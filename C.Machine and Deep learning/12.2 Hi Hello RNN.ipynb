{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teach RNN 'hihllo'\n",
    "\n",
    "RNN에게 hihello를 학습시키고 그 다음 hihell를 입력하여 ihello를 출력하는것이 목표다.\n",
    "\n",
    "즉 h를 입력하면 다음문자인 i가 나오고 i를 입력하면 다음문자인 h가 나오게 하는것이다.\n",
    "\n",
    "이것이 선형회귀같은 일반적인 forward net에서 못하는 이유는 같은 h를 넣었어도 뒤에 i나e가 나올 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 가르칠 text는 'hihello'이다.\n",
    "\n",
    "여기서 쓰인 문자를 중복없이 나열 하면 h, i e, l, o이다.\n",
    "\n",
    "따라서 각각의 문자를 index 시켜준다.\n",
    "\n",
    "     h:0    i:1    e:2   l:3    o:4\n",
    "     \n",
    "그 후 인덱스를 원핫인코딩 시켜준다.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 원하는 출력은 ihello 이므로[1, 0, 2, 3, 3, 4]을 출력해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = [[1, 0, 2, 3, 3, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 주는 x_data가 6글자 이므로 sequence_length는 6이다\n",
    "\n",
    "hidden은 출력은 우리가 원하는대로 결정할 수 있는데 원핫인코딩을 쓰므로 5여야한다.\n",
    "\n",
    "batch는 문자열이 하나이므로  1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size=5 \n",
    "input_dim=5\n",
    "batch_size=1\n",
    "sequence_length = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 RNN모델을 세워보자 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드라이브 함수로 출력을 정해주기 위해 입력의 사이즈를 정해야한다.\n",
    "\n",
    "입력(X)은 원핫인코딩을 쓰고 6글자를 사용하기 때문에 (n,6,5)여야 한다.\n",
    "\n",
    "Y는 Label로 표현되므로 반복하기때문에 (n,6)이 된다.\n",
    "\n",
    "이때 n는 배치 사이즈이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,[None,sequence_length,input_dim])\n",
    "Y=tf.placeholder(tf.int32, [None,sequence_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 cell을 만들어 보자.\n",
    "\n",
    "이때 initial state를 0으로 주자.\n",
    "\n",
    "# 왜 0으로 주는 걸까\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output이 얼마나 좋은가 loss로 계산해 봐야 한다.\n",
    "\n",
    "## cost: sequence_loss\n",
    "\n",
    "sequence_loss가 어떻게 작동하는지 알아보기 위해 한가지 예제를 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data = tf.constant([[1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 y 데이터 주어질때 값을 원핫으로 예측했다고 해보자\n",
    "\n",
    "[batch size, sequence_length, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicton = tf.constant([[[0.2,0.7],[0.6,0.2],[0.2,0.9]]],dtype=tf.float32) # 1, 0, 1로 예측\n",
    "\n",
    "weights= tf.constant([[1,1,1]],dtype=tf.float32) # 전부 1로 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 값을 sequence_loss에 대입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.596759\n"
     ]
    }
   ],
   "source": [
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=predicton, targets=sample_data, weights=weights)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Loss:\",sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 예측이 두가지 인 경우도 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.513015 \n",
      " Loss: 0.371101\n"
     ]
    }
   ],
   "source": [
    "predicton1 = tf.constant([[[0.3,0.7],[0.3,0.7],[0.3,0.7]]],dtype=tf.float32)\n",
    "predicton2 = tf.constant([[[0.1,0.9],[0.1,0.9],[0.1,0.9]]],dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(logits=predicton1, targets=sample_data, weights=weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(logits=predicton2, targets=sample_data, weights=weights)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Loss:\",sequence_loss1.eval(),\"\\n\",\"Loss:\",sequence_loss2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확하게 맞출 수록 오차가 줄어든다.\n",
    "\n",
    "이제 다시 우리 모델로 돌아와 보자\n",
    "\n",
    "weight는 전부 1로 해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.ones([batch_size,sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train= tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 해도 되지만 rnn에서 나온 값을 바로 logit에 넣으면 안된다. 이 부분에 대해서는 뒤에서 살펴보자\n",
    "\n",
    "이제 학습 시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.59796 prediction:  [[2 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "1 loss: 1.50784 prediction:  [[2 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "2 loss: 1.42344 prediction:  [[2 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "3 loss: 1.36237 prediction:  [[2 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "4 loss: 1.31126 prediction:  [[2 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "5 loss: 1.25841 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "6 loss: 1.20628 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "7 loss: 1.16079 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "8 loss: 1.13189 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "9 loss: 1.09818 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "10 loss: 1.06947 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "11 loss: 1.0464 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "12 loss: 1.02468 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "13 loss: 1.00294 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "14 loss: 0.981933 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "15 loss: 0.963172 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "16 loss: 0.946471 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "17 loss: 0.929585 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "18 loss: 0.912999 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "19 loss: 0.898327 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "20 loss: 0.885435 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "21 loss: 0.873266 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "22 loss: 0.861383 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "23 loss: 0.850122 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "24 loss: 0.839828 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "25 loss: 0.829832 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "26 loss: 0.819381 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "27 loss: 0.809209 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "28 loss: 0.800283 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "29 loss: 0.792849 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "30 loss: 0.786996 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "31 loss: 0.782768 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "32 loss: 0.77972 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "33 loss: 0.776784 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "34 loss: 0.773284 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "35 loss: 0.769606 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "36 loss: 0.766035 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "37 loss: 0.762306 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "38 loss: 0.758372 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "39 loss: 0.754457 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "40 loss: 0.750438 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "41 loss: 0.746235 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "42 loss: 0.742399 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "43 loss: 0.739213 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "44 loss: 0.736424 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "45 loss: 0.733909 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "46 loss: 0.731518 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "47 loss: 0.728802 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "48 loss: 0.725412 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "49 loss: 0.721003 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(outputs, axis=2) # 마지막 축 (aixs=2)에서 원핫 인코딩이 이루어지고, 그것중 가장 큰 값으로 예측했으므로 argmax로 뽑아온다.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrediction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "print(\"\\tPrediction str: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고사이트\n",
    "1. https://www.youtube.com/watch?v=39_P23TqUnw&index=43&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm\n",
    "2. https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-12-1-hello-rnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
