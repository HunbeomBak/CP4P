{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Regression\n",
    "데이터가 주어진다면 우리는 데이터를 보고 가설(Hypothesis)를 세울 것이다.\n",
    "\n",
    "이 가설이 선형적(Linear)한다면 H(x)=Wx+b 라고 세울 것이다.\n",
    "\n",
    "이렇게 가설을 선형적으로 세우는것을 선형회귀(Linear Regression)라고 한다.\n",
    "\n",
    "\n",
    "선형적이여서 하나의 선이 나올 것이고 그 선을 찾는것이 학습의 목표이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=[1,2,3]\n",
    "y_train=[1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 위 처럼 있을때 H(x)=Wx+b를 세울 것이다.\n",
    "\n",
    "이때 W와 b도 변수로 만들어 줘야한다. \n",
    "\n",
    "이때 tf.Variable을 사용하여 우리가 사용하는 변수가 아니라 텐서플로우가 사용하는 변수로 설정하는 것이다.\n",
    "\n",
    "(학습해서 얻어지는 변수로 선언한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b= tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리의 모델을 텐서플로우로 나타 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis =x_train*W+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost/loss function\n",
    "\n",
    "가설을 세웠다면 이 가설로 구한 값이 실제 값과 얼마나 차이나는지 확인해야한다.\n",
    "\n",
    "이때 사용하는 함수를 cost 함수 또는 loss 함수라고 하며 실제값과 구한값의 차이의 제곱을 평균낸 것이다.\n",
    "\n",
    "제곱한 이유는 구한값과 실제값의 거리를 본다고 생각하거나 음수를 양수로 만들어 주기 위해 제곱했다고 생각하면된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.square(hypothesis-y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce_mean은 자동적으로 평균을 내주는 텐서플로우의 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientDescent\n",
    "\n",
    "텐서플로우를 사용할때 (머신러닝이나 딥러닝에서) 중요한점은 cost를 최소화 시키는 것이다.\n",
    "\n",
    "GradientDescent로 최소화 시키는데, 어떻게 작동하는지는 추후에 알 수 있다, 지금은 일단 쓰자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer =tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train= optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텐서플로우 그래프를 다 만들었으니 세션을 만들어 실행시켜주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 W와 b를 사용하기 전에 global_variable_initializer로 초기화 시켜줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 train을 실행시키면 cost를 minimize하게 optimizer시킬것이고 그러기 위해 W,b를 학습을 시킬 것이다.\n",
    "\n",
    "텐서플로우 그래프에서 train에 cost가 연결되어 있고, cost에 hypothesis가, hypothesis에 W와 b가 연결되어 있다. 따라서 train을 실행하면 w와 b에 값을 저장하게 된다.\n",
    "\n",
    "단순히 한번만 실행하면 학습이 한번만 되므로 약 2000번 정도 반복하기 위해 for문을 사용한다.\n",
    "\n",
    "그리고 20번마다 그때의 횟수, cost, W, b를 출력해서 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "20 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "40 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "60 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "80 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "100 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "120 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "140 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "160 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "180 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "200 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "220 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "240 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "260 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "280 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "300 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "320 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "340 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "360 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "380 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "400 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "420 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "440 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "460 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "480 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "500 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "520 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "540 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "560 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "580 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "600 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "620 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "640 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "660 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "680 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "700 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "720 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "740 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "760 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "780 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "800 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "820 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "840 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "860 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "880 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "900 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "920 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "940 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "960 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "980 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1000 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1020 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1040 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1060 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1080 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1100 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1120 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1140 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1160 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1180 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1200 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1220 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1240 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1260 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1280 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1300 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1320 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1340 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1360 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1380 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1400 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1420 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1440 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1460 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1480 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1500 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1520 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1540 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1560 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1580 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1600 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1620 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1640 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1660 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1680 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1700 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1720 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1740 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1760 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1780 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1800 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1820 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1840 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1860 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1880 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1900 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1920 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1940 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1960 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "1980 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n",
      "2000 3.06954e-12 [ 0.99999785] [  4.35067159e-06]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20==0:\n",
    "        print(step,sess.run(cost),sess.run(W),sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder를 사용해 Linear Regression을 구현해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 모델 Hypothesis에서 H(x)= Wx+b에서 X와 Y값을 학습할때 주는 것이다.\n",
    "\n",
    "나머지 부분은 똑같으나, Hypothesis와 cost함수는 변수의 이름이 바뀌었으니 바꿔주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis =X*W+b\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 위한 for문에 feed_dict로 placeholder에 넣을 값을 정해주면 된다.\n",
    "\n",
    "for문에서 리스트로 한번에 실행 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.65673e-05 [ 0.99528402] [ 0.0107206]\n",
      "20 1.50467e-05 [ 0.99550563] [ 0.01021677]\n",
      "40 1.36652e-05 [ 0.99571687] [ 0.00973662]\n",
      "60 1.24109e-05 [ 0.99591815] [ 0.00927901]\n",
      "80 1.12723e-05 [ 0.99610996] [ 0.00884294]\n",
      "100 1.02376e-05 [ 0.99629283] [ 0.00842734]\n",
      "120 9.29776e-06 [ 0.99646699] [ 0.00803129]\n",
      "140 8.44484e-06 [ 0.99663299] [ 0.00765388]\n",
      "160 7.66921e-06 [ 0.9967913] [ 0.00729418]\n",
      "180 6.96544e-06 [ 0.9969421] [ 0.00695138]\n",
      "200 6.32594e-06 [ 0.99708575] [ 0.0066247]\n",
      "220 5.74568e-06 [ 0.99722272] [ 0.00631337]\n",
      "240 5.2183e-06 [ 0.9973532] [ 0.00601668]\n",
      "260 4.73929e-06 [ 0.99747759] [ 0.00573393]\n",
      "280 4.30414e-06 [ 0.99759614] [ 0.00546445]\n",
      "300 3.90933e-06 [ 0.99770916] [ 0.00520766]\n",
      "320 3.55041e-06 [ 0.9978168] [ 0.00496291]\n",
      "340 3.22454e-06 [ 0.99791938] [ 0.00472968]\n",
      "360 2.92873e-06 [ 0.99801719] [ 0.0045074]\n",
      "380 2.65981e-06 [ 0.99811035] [ 0.00429557]\n",
      "400 2.41551e-06 [ 0.99819916] [ 0.00409371]\n",
      "420 2.19393e-06 [ 0.9982838] [ 0.00390133]\n",
      "440 1.99273e-06 [ 0.99836445] [ 0.00371797]\n",
      "460 1.8096e-06 [ 0.99844128] [ 0.00354324]\n",
      "480 1.64369e-06 [ 0.99851447] [ 0.00337675]\n",
      "500 1.49283e-06 [ 0.99858439] [ 0.00321806]\n",
      "520 1.35573e-06 [ 0.99865097] [ 0.0030668]\n",
      "540 1.23137e-06 [ 0.99871427] [ 0.00292267]\n",
      "560 1.11817e-06 [ 0.99877471] [ 0.00278533]\n",
      "580 1.01568e-06 [ 0.99883229] [ 0.00265445]\n",
      "600 9.22492e-07 [ 0.99888712] [ 0.0025297]\n",
      "620 8.37918e-07 [ 0.99893939] [ 0.00241084]\n",
      "640 7.6089e-07 [ 0.99898928] [ 0.00229757]\n",
      "660 6.91116e-07 [ 0.99903673] [ 0.00218961]\n",
      "680 6.27685e-07 [ 0.99908197] [ 0.00208674]\n",
      "700 5.70142e-07 [ 0.99912518] [ 0.00198871]\n",
      "720 5.17778e-07 [ 0.99916625] [ 0.00189526]\n",
      "740 4.70281e-07 [ 0.99920535] [ 0.00180622]\n",
      "760 4.27128e-07 [ 0.99924278] [ 0.00172137]\n",
      "780 3.88036e-07 [ 0.99927825] [ 0.00164049]\n",
      "800 3.52352e-07 [ 0.99931222] [ 0.00156343]\n",
      "820 3.19992e-07 [ 0.99934453] [ 0.00148999]\n",
      "840 2.90682e-07 [ 0.99937528] [ 0.00141998]\n",
      "860 2.63994e-07 [ 0.99940461] [ 0.00135329]\n",
      "880 2.39764e-07 [ 0.99943256] [ 0.00128973]\n",
      "900 2.17749e-07 [ 0.99945921] [ 0.00122916]\n",
      "920 1.97808e-07 [ 0.9994846] [ 0.00117144]\n",
      "940 1.79663e-07 [ 0.9995088] [ 0.00111644]\n",
      "960 1.6321e-07 [ 0.99953187] [ 0.00106401]\n",
      "980 1.48257e-07 [ 0.9995538] [ 0.00101404]\n",
      "1000 1.34644e-07 [ 0.99957478] [ 0.00096643]\n",
      "1020 1.22284e-07 [ 0.99959475] [ 0.00092105]\n",
      "1040 1.11056e-07 [ 0.99961382] [ 0.00087785]\n",
      "1060 1.0089e-07 [ 0.99963188] [ 0.00083664]\n",
      "1080 9.16734e-08 [ 0.99964911] [ 0.00079735]\n",
      "1100 8.32227e-08 [ 0.99966568] [ 0.00075994]\n",
      "1120 7.56553e-08 [ 0.99968123] [ 0.00072432]\n",
      "1140 6.87122e-08 [ 0.99969625] [ 0.00069031]\n",
      "1160 6.24216e-08 [ 0.99971056] [ 0.00065795]\n",
      "1180 5.67005e-08 [ 0.99972403] [ 0.00062705]\n",
      "1200 5.14825e-08 [ 0.99973714] [ 0.00059765]\n",
      "1220 4.67738e-08 [ 0.99974924] [ 0.00056961]\n",
      "1240 4.24912e-08 [ 0.99976116] [ 0.00054295]\n",
      "1260 3.85789e-08 [ 0.99977225] [ 0.00051746]\n",
      "1280 3.50909e-08 [ 0.99978298] [ 0.00049323]\n",
      "1300 3.18551e-08 [ 0.99979311] [ 0.00047005]\n",
      "1320 2.89411e-08 [ 0.99980271] [ 0.00044807]\n",
      "1340 2.6281e-08 [ 0.99981213] [ 0.00042706]\n",
      "1360 2.38968e-08 [ 0.99982083] [ 0.00040702]\n",
      "1380 2.17174e-08 [ 0.99982917] [ 0.00038802]\n",
      "1400 1.96986e-08 [ 0.99983734] [ 0.00036979]\n",
      "1420 1.79061e-08 [ 0.99984485] [ 0.00035244]\n",
      "1440 1.62805e-08 [ 0.999852] [ 0.00033601]\n",
      "1460 1.47871e-08 [ 0.99985915] [ 0.0003203]\n",
      "1480 1.34453e-08 [ 0.99986571] [ 0.00030522]\n",
      "1500 1.22082e-08 [ 0.99987185] [ 0.00029096]\n",
      "1520 1.10978e-08 [ 0.99987781] [ 0.00027742]\n",
      "1540 1.00689e-08 [ 0.99988377] [ 0.00026443]\n",
      "1560 9.15733e-09 [ 0.99988919] [ 0.00025195]\n",
      "1580 8.31053e-09 [ 0.9998942] [ 0.00024016]\n",
      "1600 7.56166e-09 [ 0.99989909] [ 0.00022899]\n",
      "1620 6.88414e-09 [ 0.99990386] [ 0.00021835]\n",
      "1640 6.24647e-09 [ 0.99990857] [ 0.00020809]\n",
      "1660 5.66246e-09 [ 0.99991274] [ 0.00019826]\n",
      "1680 5.1468e-09 [ 0.99991679] [ 0.00018897]\n",
      "1700 4.67989e-09 [ 0.99992061] [ 0.00018017]\n",
      "1720 4.26699e-09 [ 0.99992424] [ 0.00017183]\n",
      "1740 3.86867e-09 [ 0.99992782] [ 0.00016387]\n",
      "1760 3.51491e-09 [ 0.9999314] [ 0.00015617]\n",
      "1780 3.19224e-09 [ 0.99993461] [ 0.00014876]\n",
      "1800 2.89598e-09 [ 0.99993765] [ 0.00014174]\n",
      "1820 2.63132e-09 [ 0.99994051] [ 0.0001351]\n",
      "1840 2.39085e-09 [ 0.9999432] [ 0.00012881]\n",
      "1860 2.17601e-09 [ 0.99994576] [ 0.00012286]\n",
      "1880 1.97955e-09 [ 0.9999482] [ 0.00011724]\n",
      "1900 1.80649e-09 [ 0.99995059] [ 0.0001119]\n",
      "1920 1.63986e-09 [ 0.99995297] [ 0.00010675]\n",
      "1940 1.48795e-09 [ 0.99995536] [ 0.00010172]\n",
      "1960 1.35199e-09 [ 0.99995756] [  9.68312233e-05]\n",
      "1980 1.22487e-09 [ 0.99995953] [  9.21947722e-05]\n",
      "2000 1.11409e-09 [ 0.99996144] [  8.78197825e-05]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    cost_value, W_value, b_value, train_=sess.run([cost,W,b,train],feed_dict={X:[1,2,3],Y:[1,2,3]})\n",
    "    if step % 20==0:\n",
    "        print(step, cost_value,W_value,b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습시켰으면 test데이터로 시험해봐야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.9998951]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict={X:[5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.49999142]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict={X:[2.5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.50003004  3.49995279]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict={X:[1.5,3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하자면 우리는 H=Wx+b라는 모델을 세우면서 cost함수까지 그래프를 그렸다.\n",
    "\n",
    "그 후 sess=tf.Session()으로 그래프를 실행하였다. 이때 placeholder로 학습할때 값을 넣어줘서 값을 받았다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
